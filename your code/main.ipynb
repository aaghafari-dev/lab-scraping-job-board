{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    \n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'}\n",
    "    \n",
    "\n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "    response = requests.get(scrape_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve the webpage. Status code:', response.status_code)\n",
    "        return print(response.status_code) \n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    job_name_element = soup.find('span', class_='sr-only')\n",
    "    job_name_list = []\n",
    "    for job in job_name_element:\n",
    "        job_name = job.get_text(strip=True)\n",
    "        job_name_list.append(job_name)\n",
    "    job_name_list = job_name_list[2:-1]\n",
    "\n",
    "    location_element = soup.find_all('span', class_='job-result-card__location')\n",
    "    location_list = []\n",
    "    for location in location_element:\n",
    "        location.append(location.get_text(strip=True))\n",
    "\n",
    "    company_element = soup.find_all('a', class_='hidden-nested-link')\n",
    "\n",
    "    company_names = [i.get_text(strip=True) for i in company_element]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Job Name': job_name_list,\n",
    "        'Company Name': company_names,\n",
    "        'Location': location_list        \n",
    "    })     \n",
    "    # Return dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Set a User-Agent header so LinkedIn doesn't block me\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "def scrape_linkedin_job_search(keywords):\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "    page = requests.get(scrape_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.base-search-card__info\"):\n",
    "        title = card.select_one(\"h3.base-search-card__title\")\n",
    "        company = card.select_one(\"h4.base-search-card__subtitle\")\n",
    "        location = card.select_one(\"span.job-search-card__location\")\n",
    "        titles.append(title.get_text(strip=True) if title else None)\n",
    "        companies.append(company.get_text(strip=True) if company else None)\n",
    "        locations.append(location.get_text(strip=True) if location else None)\n",
    "\n",
    "    data = pd.DataFrame({'Title': titles, 'Company': companies, 'Location': locations})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Associate, Strategic Finance - LinkedIn Market...</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Associate, Insights Analytics Engineer</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Associate, Insights Analytics Engineer</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Junior Quantitative Analyst - Remote</td>\n",
       "      <td>Novartis Norge</td>\n",
       "      <td>Santa Ana, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Junior Quantitative Analyst - Remote</td>\n",
       "      <td>Novartis Norge</td>\n",
       "      <td>Riverside, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Junior Quantitative Analyst - Remote</td>\n",
       "      <td>Novartis Norge</td>\n",
       "      <td>Stockton, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Business Analyst - Operations Analytics</td>\n",
       "      <td>Rover.com</td>\n",
       "      <td>Seattle, WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analysis Intern (Yearlong)</td>\n",
       "      <td>AUMOVIO</td>\n",
       "      <td>Auburn Hills, MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analytics</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Redmond, WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>San Jose, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Analyst - Entry Level</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>Dallas, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Scientist - Business Analytics &amp; ML</td>\n",
       "      <td>Kia America</td>\n",
       "      <td>Irvine, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PowerBI SQL Analyst - Remote</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Analyst Intern</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>Denver, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Paradigm Technology</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Director, Commercial Data &amp; Analytics</td>\n",
       "      <td>Krystal Biotech, Inc.</td>\n",
       "      <td>Pittsburgh, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Scientist- Applied AI</td>\n",
       "      <td>Kia America</td>\n",
       "      <td>Irvine, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Analysis &amp; Computing Tutor</td>\n",
       "      <td>Ladgov Corporation</td>\n",
       "      <td>New London, CT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Reporting Analyst</td>\n",
       "      <td>Spotnana</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Scientist, Chrome, Research</td>\n",
       "      <td>Google</td>\n",
       "      <td>Seattle, WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data Scientist, Research Operations</td>\n",
       "      <td>Google</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Analyst I</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Analyst Intern</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Manager, Product Analytics</td>\n",
       "      <td>Pinwheel</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Global Strategy and Engagement Associate</td>\n",
       "      <td>Google</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Private Equity Data Operations Analyst</td>\n",
       "      <td>LP Analyst</td>\n",
       "      <td>Dallas, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>AARATECH</td>\n",
       "      <td>Atlanta Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Health Analytics - Data Science Analyst</td>\n",
       "      <td>Aon</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Associate Data Analyst</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Analyst, Consumer Insights</td>\n",
       "      <td>Chedraui USA</td>\n",
       "      <td>Commerce, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Data Analyst - Level 2</td>\n",
       "      <td>Lockheed Martin</td>\n",
       "      <td>Fort Worth, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>E-Business Analyst-Amazon</td>\n",
       "      <td>Nestlé</td>\n",
       "      <td>Arlington, VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Kforce Inc</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Rantec Power Systems Inc.</td>\n",
       "      <td>Los Osos, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Quantitative Researcher - Early Career (USA)</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Marketing Data Scientist</td>\n",
       "      <td>Harbor Freight Tools</td>\n",
       "      <td>Calabasas, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Associate – Research COO Team</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>AARATECH</td>\n",
       "      <td>Washington DC-Baltimore Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Data Scientist II</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Data Analyst, New Grad</td>\n",
       "      <td>Jobright.ai</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Data Analyst - 100% Remote</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>Jacksonville, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Human Data Analyst | $55/hr Remote</td>\n",
       "      <td>Crossing Hurdles</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Business Intelligence Analyst (Power BI/ Power...</td>\n",
       "      <td>Kelly Science, Engineering, Technology &amp; Telecom</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Trade Marketing Analyst</td>\n",
       "      <td>Sigma</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>LTIMindtree</td>\n",
       "      <td>Irving, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>SQL Data Analyst (REMOTE EST)</td>\n",
       "      <td>Lensa</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Senior Director Data Analysis, AI Innovation</td>\n",
       "      <td>Bellwood</td>\n",
       "      <td>Atlanta Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Global Strategy and Engagement Associate</td>\n",
       "      <td>Google</td>\n",
       "      <td>Washington, DC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>AARATECH</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>AARATECH</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Senior Director Data Analysis, AI Innovation</td>\n",
       "      <td>Bellwood</td>\n",
       "      <td>Duluth, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Health Analytics - Data Science Analyst</td>\n",
       "      <td>Aon</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Analytics Associate</td>\n",
       "      <td>1915 South | Ashley</td>\n",
       "      <td>Thomasville, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Optomi</td>\n",
       "      <td>Charlotte, NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Data Scientist, Chrome, Research</td>\n",
       "      <td>Google</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Data Analyst (1881)</td>\n",
       "      <td>Kooner Fleet Management Solutions</td>\n",
       "      <td>Sacramento, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Staff, Advanced Analytics, Product</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Junior Data Engineer</td>\n",
       "      <td>Cylinder</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0   Associate, Strategic Finance - LinkedIn Market...   \n",
       "1       Senior Associate, Insights Analytics Engineer   \n",
       "2       Senior Associate, Insights Analytics Engineer   \n",
       "3                Junior Quantitative Analyst - Remote   \n",
       "4                Junior Quantitative Analyst - Remote   \n",
       "5                Junior Quantitative Analyst - Remote   \n",
       "6                                      Data Scientist   \n",
       "7             Business Analyst - Operations Analytics   \n",
       "8                     Data Analysis Intern (Yearlong)   \n",
       "9                                      Data Analytics   \n",
       "10                              Business Data Analyst   \n",
       "11                         Data Analyst - Entry Level   \n",
       "12           Data Scientist - Business Analytics & ML   \n",
       "13                       PowerBI SQL Analyst - Remote   \n",
       "14                                Data Analyst Intern   \n",
       "15                                   Business Analyst   \n",
       "16              Director, Commercial Data & Analytics   \n",
       "17                         Data Scientist- Applied AI   \n",
       "18                    Data Analysis & Computing Tutor   \n",
       "19                             Data Reporting Analyst   \n",
       "20                   Data Scientist, Chrome, Research   \n",
       "21                Data Scientist, Research Operations   \n",
       "22                                     Data Analyst I   \n",
       "23                                Data Analyst Intern   \n",
       "24                         Manager, Product Analytics   \n",
       "25           Global Strategy and Engagement Associate   \n",
       "26             Private Equity Data Operations Analyst   \n",
       "27                                       Data Analyst   \n",
       "28            Health Analytics - Data Science Analyst   \n",
       "29                             Associate Data Analyst   \n",
       "30                         Analyst, Consumer Insights   \n",
       "31                             Data Analyst - Level 2   \n",
       "32                          E-Business Analyst-Amazon   \n",
       "33                                       Data Analyst   \n",
       "34                                       Data Analyst   \n",
       "35       Quantitative Researcher - Early Career (USA)   \n",
       "36                           Marketing Data Scientist   \n",
       "37                      Associate – Research COO Team   \n",
       "38                                       Data Analyst   \n",
       "39                                       Data Analyst   \n",
       "40                                  Data Scientist II   \n",
       "41                             Data Analyst, New Grad   \n",
       "42                         Data Analyst - 100% Remote   \n",
       "43                 Human Data Analyst | $55/hr Remote   \n",
       "44  Business Intelligence Analyst (Power BI/ Power...   \n",
       "45                            Trade Marketing Analyst   \n",
       "46                                       Data Analyst   \n",
       "47                      SQL Data Analyst (REMOTE EST)   \n",
       "48       Senior Director Data Analysis, AI Innovation   \n",
       "49           Global Strategy and Engagement Associate   \n",
       "50                                   Business Analyst   \n",
       "51                                       Data Analyst   \n",
       "52       Senior Director Data Analysis, AI Innovation   \n",
       "53            Health Analytics - Data Science Analyst   \n",
       "54                                Analytics Associate   \n",
       "55                                       Data Analyst   \n",
       "56                   Data Scientist, Chrome, Research   \n",
       "57                                Data Analyst (1881)   \n",
       "58                 Staff, Advanced Analytics, Product   \n",
       "59                               Junior Data Engineer   \n",
       "\n",
       "                                             Company  \\\n",
       "0                                           LinkedIn   \n",
       "1                                           LinkedIn   \n",
       "2                                           LinkedIn   \n",
       "3                                     Novartis Norge   \n",
       "4                                     Novartis Norge   \n",
       "5                                     Novartis Norge   \n",
       "6                                           Facebook   \n",
       "7                                          Rover.com   \n",
       "8                                            AUMOVIO   \n",
       "9                                          Microsoft   \n",
       "10                                             Cisco   \n",
       "11                                             Lensa   \n",
       "12                                       Kia America   \n",
       "13                                             Lensa   \n",
       "14                                             Lensa   \n",
       "15                               Paradigm Technology   \n",
       "16                             Krystal Biotech, Inc.   \n",
       "17                                       Kia America   \n",
       "18                                Ladgov Corporation   \n",
       "19                                          Spotnana   \n",
       "20                                            Google   \n",
       "21                                            Google   \n",
       "22                                             Lensa   \n",
       "23                                             Lensa   \n",
       "24                                          Pinwheel   \n",
       "25                                            Google   \n",
       "26                                        LP Analyst   \n",
       "27                                          AARATECH   \n",
       "28                                               Aon   \n",
       "29                                             Lensa   \n",
       "30                                      Chedraui USA   \n",
       "31                                   Lockheed Martin   \n",
       "32                                            Nestlé   \n",
       "33                                        Kforce Inc   \n",
       "34                         Rantec Power Systems Inc.   \n",
       "35                                             Lensa   \n",
       "36                              Harbor Freight Tools   \n",
       "37                                    Morgan Stanley   \n",
       "38                                          AARATECH   \n",
       "39                                             Lensa   \n",
       "40                                         Microsoft   \n",
       "41                                       Jobright.ai   \n",
       "42                                             Lensa   \n",
       "43                                  Crossing Hurdles   \n",
       "44  Kelly Science, Engineering, Technology & Telecom   \n",
       "45                                             Sigma   \n",
       "46                                       LTIMindtree   \n",
       "47                                             Lensa   \n",
       "48                                          Bellwood   \n",
       "49                                            Google   \n",
       "50                                          AARATECH   \n",
       "51                                          AARATECH   \n",
       "52                                          Bellwood   \n",
       "53                                               Aon   \n",
       "54                               1915 South | Ashley   \n",
       "55                                            Optomi   \n",
       "56                                            Google   \n",
       "57                 Kooner Fleet Management Solutions   \n",
       "58                                            Airbnb   \n",
       "59                                          Cylinder   \n",
       "\n",
       "                        Location  \n",
       "0                   New York, NY  \n",
       "1                   New York, NY  \n",
       "2                    Chicago, IL  \n",
       "3                  Santa Ana, CA  \n",
       "4                  Riverside, CA  \n",
       "5                   Stockton, CA  \n",
       "6                  United States  \n",
       "7                    Seattle, WA  \n",
       "8               Auburn Hills, MI  \n",
       "9                    Redmond, WA  \n",
       "10                  San Jose, CA  \n",
       "11                    Dallas, TX  \n",
       "12                    Irvine, CA  \n",
       "13                    Denver, CO  \n",
       "14                    Denver, CO  \n",
       "15                 United States  \n",
       "16                Pittsburgh, PA  \n",
       "17                    Irvine, CA  \n",
       "18                New London, CT  \n",
       "19                 United States  \n",
       "20                   Seattle, WA  \n",
       "21                 Sunnyvale, CA  \n",
       "22                   Phoenix, AZ  \n",
       "23                   Chicago, IL  \n",
       "24                  New York, NY  \n",
       "25                  New York, NY  \n",
       "26                    Dallas, TX  \n",
       "27     Atlanta Metropolitan Area  \n",
       "28                   Chicago, IL  \n",
       "29                  New York, NY  \n",
       "30                  Commerce, CA  \n",
       "31                Fort Worth, TX  \n",
       "32                 Arlington, VA  \n",
       "33                    Boston, MA  \n",
       "34                  Los Osos, CA  \n",
       "35                  New York, NY  \n",
       "36                 Calabasas, CA  \n",
       "37                  New York, NY  \n",
       "38  Washington DC-Baltimore Area  \n",
       "39                  New York, NY  \n",
       "40                 United States  \n",
       "41                 United States  \n",
       "42              Jacksonville, FL  \n",
       "43                 United States  \n",
       "44                 United States  \n",
       "45                 United States  \n",
       "46                    Irving, TX  \n",
       "47                    Boston, MA  \n",
       "48     Atlanta Metropolitan Area  \n",
       "49                Washington, DC  \n",
       "50                  New York, NY  \n",
       "51        San Francisco Bay Area  \n",
       "52                    Duluth, GA  \n",
       "53                   Atlanta, GA  \n",
       "54               Thomasville, GA  \n",
       "55                 Charlotte, NC  \n",
       "56             San Francisco, CA  \n",
       "57                Sacramento, CA  \n",
       "58                 United States  \n",
       "59                 United States  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to call the function\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}\n",
    "\n",
    "def scrape_linkedin_job_search(keywords, num_pages=1):\n",
    "    \n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    all_titles = []\n",
    "    all_companies = []\n",
    "    all_locations = []\n",
    "\n",
    "    for page in range(num_pages):\n",
    "        start = page * 25\n",
    "        scrape_url = f\"{BASE_URL}keywords={keywords}&start={start}\"\n",
    "        \n",
    "        response = requests.get(scrape_url, headers=HEADERS)\n",
    "        if response.status_code != 200:\n",
    "            break  # Stop if the request fails\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cards = soup.select(\"div.base-search-card__info\")\n",
    "        if not cards:\n",
    "            break  # No more job listings, stop pagination\n",
    "        \n",
    "        for card in cards:\n",
    "            title = card.select_one(\"h3.base-search-card__title\")\n",
    "            company = card.select_one(\"h4.base-search-card__subtitle\")\n",
    "            location = card.select_one(\"span.job-search-card__location\")\n",
    "            all_titles.append(title.get_text(strip=True) if title else None)\n",
    "            all_companies.append(company.get_text(strip=True) if company else None)\n",
    "            all_locations.append(location.get_text(strip=True) if location else None)\n",
    "        \n",
    "        time.sleep(1)  # Polite delay between requests\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'Title': all_titles,\n",
    "        'Company': all_companies,\n",
    "        'Location': all_locations\n",
    "    })\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Associate, Strategic Finance - LinkedIn Market...</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Associate, Insights Analytics Engineer</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Associate, Insights Analytics Engineer</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Analyst - Operations Analytics</td>\n",
       "      <td>Rover.com</td>\n",
       "      <td>Seattle, WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Junior Quantitative Analyst - Remote</td>\n",
       "      <td>Novartis Norge</td>\n",
       "      <td>Santa Ana, CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title         Company  \\\n",
       "0  Associate, Strategic Finance - LinkedIn Market...        LinkedIn   \n",
       "1      Senior Associate, Insights Analytics Engineer        LinkedIn   \n",
       "2      Senior Associate, Insights Analytics Engineer        LinkedIn   \n",
       "3            Business Analyst - Operations Analytics       Rover.com   \n",
       "4               Junior Quantitative Analyst - Remote  Novartis Norge   \n",
       "\n",
       "        Location  \n",
       "0   New York, NY  \n",
       "1   New York, NY  \n",
       "2    Chicago, IL  \n",
       "3    Seattle, WA  \n",
       "4  Santa Ana, CA  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with 5 pages\n",
    "df = scrape_linkedin_job_search(\"data analysis\", num_pages=5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}\n",
    "\n",
    "def scrape_linkedin_job_search(keywords, num_pages=1, country=None):\n",
    "    \n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    all_titles = []\n",
    "    all_companies = []\n",
    "    all_locations = []\n",
    "\n",
    "    for page in range(num_pages):\n",
    "        start = page * 25\n",
    "        # Base URL with keywords and start parameter\n",
    "        url = f\"{BASE_URL}keywords={quote(keywords)}&start={start}\"\n",
    "        # Append country filter if provided\n",
    "        if country:\n",
    "            url += f\"&location={quote(country)}\"\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code != 200:\n",
    "            break  # Stop if the request fails\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cards = soup.select(\"div.base-search-card__info\")\n",
    "        if not cards:\n",
    "            break  # No more job listings, stop pagination\n",
    "        \n",
    "        for card in cards:\n",
    "            title = card.select_one(\"h3.base-search-card__title\")\n",
    "            company = card.select_one(\"h4.base-search-card__subtitle\")\n",
    "            location = card.select_one(\"span.job-search-card__location\")\n",
    "            all_titles.append(title.get_text(strip=True) if title else None)\n",
    "            all_companies.append(company.get_text(strip=True) if company else None)\n",
    "            all_locations.append(location.get_text(strip=True) if location else None)\n",
    "        \n",
    "        time.sleep(1)  # Polite delay between requests\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'Title': all_titles,\n",
    "        'Company': all_companies,\n",
    "        'Location': all_locations\n",
    "    })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 279 jobs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Junior Credit Decisions Analyst (m/f/d)</td>\n",
       "      <td>YouLend</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analysis Internship</td>\n",
       "      <td>Lucentra Group</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Cognizant Netcentric</td>\n",
       "      <td>Frankfurt am Main, Hesse, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science Intern (f/m/x)</td>\n",
       "      <td>Enpal</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Junior Data Analyst (m/f/d)</td>\n",
       "      <td>Westwing</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title               Company  \\\n",
       "0  Junior Credit Decisions Analyst (m/f/d)               YouLend   \n",
       "1                 Data Analysis Internship        Lucentra Group   \n",
       "2                             Data Analyst  Cognizant Netcentric   \n",
       "3              Data Science Intern (f/m/x)                 Enpal   \n",
       "4              Junior Data Analyst (m/f/d)              Westwing   \n",
       "\n",
       "                            Location  \n",
       "0            Berlin, Berlin, Germany  \n",
       "1            Berlin, Berlin, Germany  \n",
       "2  Frankfurt am Main, Hesse, Germany  \n",
       "3            Berlin, Berlin, Germany  \n",
       "4           Munich, Bavaria, Germany  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with 5 pages in a specific country\n",
    "df = scrape_linkedin_job_search(\"data analysis\", num_pages=5, country=\"Germany\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}\n",
    "\n",
    "def scrape_linkedin_job_search(keywords, num_pages=1, country=None, num_days=None):\n",
    "    \n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    all_titles = []\n",
    "    all_companies = []\n",
    "    all_locations = []\n",
    "\n",
    "    for page in range(num_pages):\n",
    "        start = page * 25\n",
    "        # Build base URL with keywords and start\n",
    "        url = f\"{BASE_URL}keywords={quote(keywords)}&start={start}\"\n",
    "        \n",
    "        # Add country filter if provided\n",
    "        if country:\n",
    "            url += f\"&location={quote(country)}\"\n",
    "        \n",
    "        # Add time filter if num_days is provided\n",
    "        if num_days is not None:\n",
    "            seconds = num_days * 24 * 60 * 60\n",
    "            url += f\"&f_TPR=r{seconds}\"\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code != 200:\n",
    "            break  # Stop if the request fails\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cards = soup.select(\"div.base-search-card__info\")\n",
    "        if not cards:\n",
    "            break  # No more job listings, stop pagination\n",
    "        \n",
    "        for card in cards:\n",
    "            title = card.select_one(\"h3.base-search-card__title\")\n",
    "            company = card.select_one(\"h4.base-search-card__subtitle\")\n",
    "            location = card.select_one(\"span.job-search-card__location\")\n",
    "            all_titles.append(title.get_text(strip=True) if title else None)\n",
    "            all_companies.append(company.get_text(strip=True) if company else None)\n",
    "            all_locations.append(location.get_text(strip=True) if location else None)\n",
    "        \n",
    "        time.sleep(1)  # Polite delay between requests\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'Title': all_titles,\n",
    "        'Company': all_companies,\n",
    "        'Location': all_locations\n",
    "    })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 285 jobs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business-Analyst</td>\n",
       "      <td>Instaffo</td>\n",
       "      <td>Frechen, North Rhine-Westphalia, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior Credit Decisions Analyst (m/f/d)</td>\n",
       "      <td>YouLend</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analysis Internship</td>\n",
       "      <td>Lucentra Group</td>\n",
       "      <td>Berlin, Berlin, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Internship - Data &amp; Analytics (m/f/x)</td>\n",
       "      <td>FINN</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Cognizant Netcentric</td>\n",
       "      <td>Frankfurt am Main, Hesse, Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title               Company  \\\n",
       "0                         Business-Analyst              Instaffo   \n",
       "1  Junior Credit Decisions Analyst (m/f/d)               YouLend   \n",
       "2                 Data Analysis Internship        Lucentra Group   \n",
       "3    Internship - Data & Analytics (m/f/x)                  FINN   \n",
       "4                             Data Analyst  Cognizant Netcentric   \n",
       "\n",
       "                                   Location  \n",
       "0  Frechen, North Rhine-Westphalia, Germany  \n",
       "1                   Berlin, Berlin, Germany  \n",
       "2                   Berlin, Berlin, Germany  \n",
       "3                  Munich, Bavaria, Germany  \n",
       "4         Frankfurt am Main, Hesse, Germany  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with 5 pages, in Germany, posted in last 7 days\n",
    "df = scrape_linkedin_job_search(\"data analysis\", num_pages=5, country=\"Germany\", num_days=7)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
